{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCQ46VME7NFQ+17ac6IEeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "706b2202e1a64badbc0bd5aff557dfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3c1a3739ef949878bcfad679b67b4d8",
              "IPY_MODEL_c5729597ebf9417f8c3e70b79df0aaa7",
              "IPY_MODEL_287a0bfd77b44cc28349ce828ff84b04"
            ],
            "layout": "IPY_MODEL_5789b502bd514f5495e58b748e3b38d8"
          }
        },
        "c3c1a3739ef949878bcfad679b67b4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b24f5f0b4c408daa2e9fdd1311bb84",
            "placeholder": "​",
            "style": "IPY_MODEL_5e6d48b0b5e1428c93924190460cdc5f",
            "value": "  0%"
          }
        },
        "c5729597ebf9417f8c3e70b79df0aaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c9c5cc61864ea5bb921bf1d71a6208",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69682e91a2da4e78b9b802f3521023d5",
            "value": 0
          }
        },
        "287a0bfd77b44cc28349ce828ff84b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e97e04320bcf48cb84ea9acbfb5fdd07",
            "placeholder": "​",
            "style": "IPY_MODEL_35707f4a96e245998f492506188ca6c4",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "5789b502bd514f5495e58b748e3b38d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b24f5f0b4c408daa2e9fdd1311bb84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e6d48b0b5e1428c93924190460cdc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64c9c5cc61864ea5bb921bf1d71a6208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69682e91a2da4e78b9b802f3521023d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e97e04320bcf48cb84ea9acbfb5fdd07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35707f4a96e245998f492506188ca6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rageya/food-vision/blob/main/FoodVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5XW76-LR-gZ"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3J2SlgXDSs9K",
        "outputId": "42ba4c84-a455-4284-aca4-4c5db7e4157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai9z-PyBS0-1",
        "outputId": "ebed5e84-d761-4041-aee4-52206803b407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = data_20_percent_path / 'train'\n",
        "test_dir = data_20_percent_path / 'test'"
      ],
      "metadata": {
        "id": "n_O3w9xrYzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an EffNetB2 faeture extractor\n",
        "\n",
        "Feature extractor : a term for a trasnfer learning model that has its base layers frozen and output or head layers customized to a cetain problem."
      ],
      "metadata": {
        "id": "R3W9D1zEY9T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # best avaialable weights\n",
        "\n",
        "# get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "#3 setup pretrained model instance\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights).to(device)\n",
        "\n",
        "# freeze the base layers in the model(will stop all layers from training)\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "0shSXnNyaLmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByPnbtzvbNnl",
        "outputId": "a6e8fae8-4259-4937-879b-d494d4d3abf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seeds for reroducibility\n",
        "set_seeds()\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3,\n",
        "               inplace=True),\n",
        "    nn.Linear(in_features=1408,\n",
        "              out_features=3,\n",
        "              bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "BawDCNb6b1jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(effnetb2,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],input_size=(1, 3, 224, 224),\n",
        "         col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXmeeCUhcOoq",
        "outputId": "0844b173-9da2-4afd-8d6a-2897e0bf4c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (Units.MEGABYTES): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    # Changed: Use torchvision.models instead of torchvision.model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "qXfcQGB_cjSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Creating Dataloaders for EffNetB2"
      ],
      "metadata": {
        "id": "jquPhbCddti7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform =effnetb2_transforms,\n",
        "                                                                                                 batch_size= 32)"
      ],
      "metadata": {
        "id": "Z5UVg3pHd9gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training an Effnetb2 model\n"
      ],
      "metadata": {
        "id": "ZWI6tz9hecNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# loss fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                              lr=1e-3)\n",
        "\n",
        "# training function\n",
        "set_seeds()\n",
        "effnet_results = engine.train(model=effnetb2,\n",
        "                              train_dataloader=train_dataloader_effnetb2,\n",
        "                              test_dataloader = test_dataloader_effnetb2,\n",
        "                              epochs=10,\n",
        "                              optimizer=optimizer,\n",
        "                              loss_fn=loss_fn,\n",
        "                              device=device)"
      ],
      "metadata": {
        "id": "bw2o0cMhfeGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "706b2202e1a64badbc0bd5aff557dfeb",
            "c3c1a3739ef949878bcfad679b67b4d8",
            "c5729597ebf9417f8c3e70b79df0aaa7",
            "287a0bfd77b44cc28349ce828ff84b04",
            "5789b502bd514f5495e58b748e3b38d8",
            "e5b24f5f0b4c408daa2e9fdd1311bb84",
            "5e6d48b0b5e1428c93924190460cdc5f",
            "64c9c5cc61864ea5bb921bf1d71a6208",
            "69682e91a2da4e78b9b802f3521023d5",
            "e97e04320bcf48cb84ea9acbfb5fdd07",
            "35707f4a96e245998f492506188ca6c4"
          ]
        },
        "outputId": "c6461d9d-cf46-4440-aa5f-6585e0874ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "706b2202e1a64badbc0bd5aff557dfeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "train_step() missing 1 required positional argument: 'epochs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-071736ec99ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m effnet_results = engine.train(model=effnetb2,\n\u001b[0m\u001b[1;32m     13\u001b[0m                               \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader_effnetb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                               \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataloader_effnetb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/going_modular/going_modular/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# Loop through training and testing steps for a number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         train_loss, train_acc = train_step(model=model,\n\u001b[0m\u001b[1;32m    171\u001b[0m                                           \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                                           \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train_step() missing 1 required positional argument: 'epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting EffNetb2 Loss curves.."
      ],
      "metadata": {
        "id": "leh71slpSNlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(effnet_results)"
      ],
      "metadata": {
        "id": "I8ng0Dr8Su93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving EffNetB2 feature extractor\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir='models',\n",
        "                 model_name='pretrained_effnetb2_feature_extractor_20%.pth')"
      ],
      "metadata": {
        "id": "KCijSY8kSxk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the size o f our EffNetB2 feature extractor.."
      ],
      "metadata": {
        "id": "GeqpiVqYUCbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "# model size in bytes and convert to megabytes..\n",
        "file_path = \"models/pretrained_effnetb2_feature_extractor_20%.pth\"\n",
        "pretrained_effnetb2_model_size = os.stat(file_path).st_size / (1024 * 1024)\n",
        "print(f'Pretrained EffNetB2 model size: {pretrained_effnetb2_model_size:.2f} MB')"
      ],
      "metadata": {
        "id": "jP0CX1c4UIKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting EffNetB2 feature extractor stats"
      ],
      "metadata": {
        "id": "sOoQTxnKUz6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "RwmvsYfAWXEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to dictionary\n",
        "effnetb2_stats = {\"test_loss\" : effnet_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\" : effnet_results[\"test_acc\"][-1],\n",
        "                  \"number of parameters\" : effnetb2_total_params,\n",
        "                  \"model size in MB\" : pretrained_effnetb2_model_size}\n",
        "\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "YKWa93GWWnDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a ViT feature extractor"
      ],
      "metadata": {
        "id": "sLuPuxYUXnGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out ViT's heads layers\n",
        "\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads\n"
      ],
      "metadata": {
        "id": "p-JDqVbqX6LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "    # Create ViT_B_16 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "    # Freeze all layers in model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head to suit our needs (this will be trainable)\n",
        "    torch.manual_seed(seed)\n",
        "    model.heads = nn.Sequential(nn.Linear(in_features=768,\n",
        "                                          out_features=num_classes))\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "8yGmenD7Y_M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit, vit_transforms = create_vit_model()"
      ],
      "metadata": {
        "id": "oaXMNm8wZ-Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_transforms"
      ],
      "metadata": {
        "id": "acphtdPfaCRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "  # Print ViT feature extractor model summary (uncomment for full output)\n",
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "1oQnxgwHasqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dataloaders for vit feature extractor"
      ],
      "metadata": {
        "id": "FeZuQjljbLER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=vit_transforms,\n",
        "                                                                               batch_size=32)"
      ],
      "metadata": {
        "id": "nSkHdl_mbW8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT model with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "KUih3aiRbtUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss curves\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "XzsFqxMAcM3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### aving Vit feature xetractor\n",
        "\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir='models',\n",
        "                 model_name='pretrained_vit_feature_extractor_20%.pth')"
      ],
      "metadata": {
        "id": "ixzAPjVVdbE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# get the model size in megabytes\n",
        "file_path = Path(\"models/pretrained_vit_feature_extractor_20%.pth\")\n",
        "pretrain_vit_feature_extractor_model_size = Path(file_path).stat().st_size / (1024 * 1024)\n",
        "print(f'Pre trained ViT feature extractor model size: {pretrain_vit_feature_extractor_model_size:.2f} MB')"
      ],
      "metadata": {
        "id": "6mVB6jbbembl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting Vit feature extractor stats"
      ],
      "metadata": {
        "id": "14FtS5bWe01c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count total number of params\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "f2zJ_yfMe-Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ViT stat dictionary\n",
        "vit_stats = {\"test_loss\" : vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\" : vit_results[\"test_acc\"][-1],\n",
        "             \"number of parameters\" : vit_total_params,\n",
        "             \"model size in MB\" : pretrain_vit_feature_extractor_model_size}\n",
        "\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "BCT4PNV3fEjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "X4PBW-dAfip1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making prediction with out trained model and timing.."
      ],
      "metadata": {
        "id": "DwkUqe12ftMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# get all test data paths\n",
        "test_data_paths  = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:10]"
      ],
      "metadata": {
        "id": "6g2cFaZif_9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creatin a function to make accros the test dataset"
      ],
      "metadata": {
        "id": "JCxQJJ0ZgrV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from typing import List, Dict\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_Names: List[str],\n",
        "                   device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  # create an empty list\n",
        "  pred_list = []\n",
        "\n",
        "  # loop throught the input paths\n",
        "  for path in tqdm(paths):\n",
        "\n",
        "    pred_dict = {}\n",
        "\n",
        "    # get the sample path and ground truth\n",
        "    pred_dict['image_path'] = path\n",
        "    class_Name = path.parent.stem\n",
        "    pred_dict['class_name'] = class_Name\n",
        "\n",
        "    # sart the pred timer\n",
        "    start_timer = timer()\n",
        "\n",
        "    # open the image\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # transform the image to be usabele with a given model\n",
        "    transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # TUrn on inference mode\n",
        "    with torch.inference_mode():\n",
        "      pred_logits = model(transformed_image)\n",
        "      pred_prob = torch.softmax(pred_logits, dim=1)\n",
        "      pred_label = torch.argmax(pred_prob, dim=1)\n",
        "      pred_class = class_names[pred_label.cpu()] # hardcode pred class to be on cpu as python varibales live on cpu\n",
        "\n",
        "      # add pred prob + pred class to empty dict\n",
        "      pred_dict['pred_prob'] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "      pred_dict['pred_class'] = pred_class\n",
        "\n",
        "      # ending the pred timer\n",
        "      end_timer = timer()\n",
        "      pred_dict['time_for_pred'] = round(end_timer-start_timer, 4)\n",
        "\n",
        "    # see if pred class matches the ground truth class\n",
        "    pred_dict['correct'] = class_Name == pred_class\n",
        "\n",
        "    # add pred dict to pred list\n",
        "    pred_list.append(pred_dict)\n",
        "\n",
        "  return pred_list"
      ],
      "metadata": {
        "id": "Kb8XY7zQg3Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making and timing pred with EffNetB2"
      ],
      "metadata": {
        "id": "5_VrsdK6kv4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions across test dataset with EffNetB2\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_Names=class_names,\n",
        "                                          device=\"cpu\") # make predictions on CPU"
      ],
      "metadata": {
        "id": "pGHK9N_Zk_y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "effnetb2_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "iwGUoEoZlAK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "MPV2inUrl7yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "e_XenMzzl-LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per prediction\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "sq5enhIdmBC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add EffNetB2 average prediction time to stats dictionary\n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "umNmgfzamDFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making timing and pred with ViT"
      ],
      "metadata": {
        "id": "ybHReA7umFz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make list of prediction dictionaries with ViT feature extractor model on test images\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                     model=vit,\n",
        "                                     transform=vit_transforms,\n",
        "                                     class_Names=class_names,\n",
        "                                     device=\"cpu\")"
      ],
      "metadata": {
        "id": "lw8hK2dAmM3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT predictions on the test dataset\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "ybU-51O6mNWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "eLvSoKa9mUVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of correct predictions\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "LfrN9U9XmVcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time per prediction for ViT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "eGCYkHjemWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "WwqHm96ZmXq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into DataFrame\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
        "\n",
        "# Convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hUjb2VccKwwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "bApqloKUKzes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vit VS EffNetB2"
      ],
      "metadata": {
        "id": "yqeTOo5fmY79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 1. Create a plot from model comparison DataFrame\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
        "                     s=df[\"model size in MB\"]) # size the dots by the model sizes\n",
        "\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis='both', labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "#Annotate with model names\n",
        "for index, row in df.iterrows():\n",
        "    ax.annotate(text=row[\"model\"],\n",
        "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
        "                size=12)\n",
        "\n",
        "#Create a legend based on model sizes\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"lower right\",\n",
        "                              title=\"Model size (MB)\",\n",
        "                              fontsize=12)\n",
        "# Create the 'images' directory if it doesn't exist\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H8AzwBOJVNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a GRADIO demo"
      ],
      "metadata": {
        "id": "aZsXRDS5K0YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "zGE1Z0oyL9qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating a function to map inputs and outputs.."
      ],
      "metadata": {
        "id": "EriuZBEwMv43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put our model on cpu\n",
        "effnetb2 = effnetb2.to('cpu')\n",
        "\n",
        "# check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "jG2d4Cl_Ne6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images of food -> ML model -> output\n",
        "from typing import Tuple, Dict\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "\n",
        "  # start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # transform input image for use with effnetb2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "  # put model into eval mode and make pred\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred_probs = torch.softmax(effnetb2(img),dim=1)\n",
        "\n",
        "  # create a pred label and pred prob dict\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # calculate the pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time\n"
      ],
      "metadata": {
        "id": "NhvXMJrgNuP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import ImageChops\n",
        "# get a list of all test images filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[0]\n",
        "\n",
        "#randomly slesct a test image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "random_image_path\n",
        "\n",
        "#open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f'Predictiing on image at path : {random_image_path}')\n",
        "\n",
        "# predict on target image and print outputs\n",
        "pred_dict, pred_time = predict(image)\n",
        "print(pred_dict)\n",
        "print(pred_time)"
      ],
      "metadata": {
        "id": "yfxNcUfFPH9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a list of example images"
      ],
      "metadata": {
        "id": "nXR0Xw_vPn7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "5VOTbjBBQuOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Gradio interface"
      ],
      "metadata": {
        "id": "IuKqoMmSQ_9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# create title, desc and article\n",
        "title=\"Food Vision Mini 🍕🥩🍣\"\n",
        "description=\"An EffNetB2 feature extractor computer vision model to classify image of Pizza, Steak, Sushi...\"\n",
        "article=\"Soon\"\n",
        "\n",
        "# creating the gradio demo\n",
        "demo = gr.Interface(fn=predict,\n",
        "                    inputs=gr.Image(type='pil'),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (S)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False,\n",
        "            share=True) # generates a publically shareable URL"
      ],
      "metadata": {
        "id": "vgPF-kxVRQIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permanently deploying Food Vision Mini using HugginFace"
      ],
      "metadata": {
        "id": "qKCs-FUJSv5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a `demos` folder to store app files"
      ],
      "metadata": {
        "id": "PTapBZ3yVP-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# create foodvision min demo path\n",
        "foodvision_mini_demo_path = Path('demos/foodvision_mini')\n",
        "\n",
        "# remove files taht might exist and create a new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "  shutil.rmtree(foodvision_mini_demo_path)\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                  exist_ok=True)\n",
        "else:\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                  exist_ok=True)\n",
        "  !ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "dgXqL9s_sFiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a folder of example images to use with foodvision mini demo"
      ],
      "metadata": {
        "id": "gWujhc_osdss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "#create example directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / 'examples'\n",
        "foodvision_mini_examples_path.mkdir(parents=True,\n",
        "                                exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# 3. Copy the three random images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "    destination = foodvision_mini_examples_path / example.name\n",
        "    print(f\"[INFO] Copying {example} to {destination}\")\n",
        "    shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "id": "2Ie9gv7QtO4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "FtOYLuScusv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moving our trained EffNetB2 model to our foodvision mini demo directory.."
      ],
      "metadata": {
        "id": "4jwbR6Dktllm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# create source path for out target model\n",
        "effnetb2_foodvision_mini_model_path = 'models/pretrained_effnetb2_feature_extractor_20%.pth'\n",
        "\n",
        "# create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# trying to move the model file\n",
        "try:\n",
        "  print(f'[INFO] Moving {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}')\n",
        "\n",
        "  #move the model\n",
        "  shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "              dst=effnetb2_foodvision_mini_model_destination)\n",
        "  print(f'Model move completed successfully')\n",
        "except:\n",
        "  print(f'[INFO] No model found at : {effnetb2_foodvision_mini_model_path}, perhaps it already ahs been moved')\n",
        "  print(f'Model exists at : {effnetb2_foodvision_mini_model_destination} : {effnetb2_foodvision_mini_model_destination.exists()}')"
      ],
      "metadata": {
        "id": "nr9xmW_IvsqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning of EffNetB2 model into a python script"
      ],
      "metadata": {
        "id": "8myoinwMwNuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "nRNOrmDwxlzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from demos.foodvision_mini import model\n",
        "\n",
        "effnetb2_model, effnetb2_transforms_import = model.create_effnetb2_model()\n",
        "effnetb2_model"
      ],
      "metadata": {
        "id": "VVcIk724yMv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning out foodvision mini gradio app into python script"
      ],
      "metadata": {
        "id": "Jydcjmwxyjoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# setup class names\n",
        "class_names = ['pizza', 'steak', 'sushi']\n",
        "\n",
        "# model and transforms preparation\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3)\n",
        "\n",
        "# laod save weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "# predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "\n",
        "  # start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # transform input image for use with effnetb2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "  # put model into eval mode and make pred\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred_probs = torch.softmax(effnetb2(img),dim=1)\n",
        "\n",
        "  # create a pred label and pred prob dict\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # calculate the pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "## Gradio APP\n",
        "\n",
        "# create title, desc and article\n",
        "title=\"Food Vision Mini 🍕🥩🍣\"\n",
        "description=\"An EffNetB2 feature extractor computer vision model to classify image of Pizza, Steak, Sushi...\"\n",
        "article=\"Soon\"\n",
        "\n",
        "#create example list\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir('examples')]\n",
        "\n",
        "# creating the gradio demo\n",
        "demo = gr.Interface(fn=predict,\n",
        "                    inputs=gr.Image(type='pil'),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (S)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False,\n",
        "            share=True) # generates a publically shareable URL"
      ],
      "metadata": {
        "id": "ZQlNoyPo2PNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating `requirements.txt`"
      ],
      "metadata": {
        "id": "Pr1reRk74y-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch==2.5.1+cu121\n",
        "torchvision==0.20.1+cu121\n",
        "gradio==5.13.1"
      ],
      "metadata": {
        "id": "NnnjDV6O5D3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying our model on Hugging face Spaces"
      ],
      "metadata": {
        "id": "LsnmEqMh5alZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading our foodvision mini app files\n",
        "!ls demos/foodvision_mini/"
      ],
      "metadata": {
        "id": "Nd481S9s6C6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change into foodvision min directory and zip it from inside\n",
        "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\""
      ],
      "metadata": {
        "id": "fRm_9WHw6aj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download it locally\n",
        "#from google.colab import files\n",
        "#try:\n",
        "#  files.download('demos/foodvision_mini.zip')\n",
        "#except:\n",
        "#  print('Not running on Google Colab')"
      ],
      "metadata": {
        "id": "UvYRsje87eas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IPython is a library to help make Python interactive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# Embed FoodVision Mini Gradio demo\n",
        "IFrame(src=\"https://huggingface.co/spaces/Rageya/foodvision/+\", width=900, height=750)"
      ],
      "metadata": {
        "id": "rldkm8Z48dM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FoodVision Big model on Food101 Dataset.."
      ],
      "metadata": {
        "id": "N0qZX-vz_-wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creaiing a model"
      ],
      "metadata": {
        "id": "ahMVHuQbADdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_101_model, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "8s71jHi9Aea8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        " # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n",
        "summary(effnetb2_101_model,\n",
        "  input_size=(1, 3, 224, 224),\n",
        "  col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "  col_width=20,\n",
        "  row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "DRDkOo1QA2gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food101_train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms\n",
        "])"
      ],
      "metadata": {
        "id": "pnzp7Fe1CMOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Getting data"
      ],
      "metadata": {
        "id": "5cIjE32gEIEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "#setup path directory\n",
        "from pathlib import Path\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "#get the training data\n",
        "train_data = datasets.Food101(root=data_dir,\n",
        "                              split=\"train\",\n",
        "                              transform=food101_train_transforms,\n",
        "                              download=True)\n",
        "\n",
        "#getting the testing data\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                             split=\"test\",\n",
        "                             transform=effnetb2_transforms,\n",
        "                             download=True)"
      ],
      "metadata": {
        "id": "dfnIQnuMEJb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the classes\n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "tdWvekeNFazF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataloaders"
      ],
      "metadata": {
        "id": "iH2oE5cnHkCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "HHLEsFBpNDLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,          # Number of epochs with no improvement before stopping\n",
        "    min_delta=0.01,      # Minimum change in the monitored quantity to qualify as an improvement\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")"
      ],
      "metadata": {
        "id": "_BoavUokNF06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4\n",
        "\n",
        "# Create Food101 training DataLoader\n",
        "train_dataloader_food101 = torch.utils.data.DataLoader(train_data,\n",
        "                                                                  batch_size=BATCH_SIZE,\n",
        "                                                                  shuffle=True,\n",
        "                                                                  num_workers=NUM_WORKERS)\n",
        "# Create Food101 testing DataLoader\n",
        "test_dataloader_food101 = torch.utils.data.DataLoader(test_data,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 shuffle=False,\n",
        "                                                                 num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "3sbVOlyRH7_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model.."
      ],
      "metadata": {
        "id": "YeLgXW43LFIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "#optimizer\n",
        "optimizer=torch.optim.Adam(params=effnetb2_101_model.parameters(),\n",
        "                           lr=1e-3)\n",
        "\n",
        "#loss fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#train the model\n",
        "effnetb2_101_model_results = engine.train(model=effnetb2_101_model,\n",
        "                                          train_dataloader=train_dataloader_food101,\n",
        "                                          test_dataloader=test_dataloader_food101,\n",
        "                                          optimizer=optimizer,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          epochs=16,\n",
        "                                          device=device)"
      ],
      "metadata": {
        "id": "sEe2LlBhLHCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8SEJVNpn7y-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}